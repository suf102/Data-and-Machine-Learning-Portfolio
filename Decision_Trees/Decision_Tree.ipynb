{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree for classification\n",
    "In this notebook I will bring int he iris dataset and use it to create a decision tree algorithem that will classify the different flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will cheat a little and bring in scikitlearn just to pull the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I will be using here is the iris dataset, for no perticular reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df['label'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to make the decision tree. I am going to make this decision tree in an object orientated fashion. It is easier in this case to make things object orientated to have a decision tree object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First the nodes \n",
    "class node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None,right=None,info_gain=None,value=None):\n",
    "        \n",
    "        #for decision nodes\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain \n",
    "        \n",
    "        #for the leaves \n",
    "        \n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then the tree itself \n",
    "class Tree_classifier():\n",
    "    def __init__(self,min_samples_split =2 ,max_depth = 2):\n",
    "        \n",
    "        #initializing the tree itself\n",
    "        \n",
    "        self.root = None \n",
    "        \n",
    "        # Stopping conditions\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth \n",
    "        \n",
    "    def build_tree(self,dataset, curr_depth = 0):\n",
    "        #building the tree recursively itterating through the levels ensuring at each level there is a left and right split\n",
    "        X = dataset[:,:-1]\n",
    "        Y = dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # Making sure the tree only runs while there are enough samples or we are above the max depth\n",
    "        \n",
    "        if num_samples >=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # then we want to find the best split for the data we have at the level we are at\n",
    "            best_split = self.get_best_split(dataset,num_samples,num_features)\n",
    "            #Make sure that the split that we do make increases the amount of information we have\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                #Building the left and right subtrees \n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"],curr_depth+1 )\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"],curr_depth+1 )\n",
    "                # Returning the node as a object from above\n",
    "                return node(best_split[\"feature_index\"], best_split[\"threshold\"], left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "                \n",
    "        # if on the other hand we are at out final depth, instead we will create the leaf nodes\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        \n",
    "        return node(value= leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset,num_samples, num_features):\n",
    "        # for each non leaf node, finding out what the best thresholds are to split the data\n",
    "        # First make somewhere to store the best split \n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # Loop over all of the features to find the best split\n",
    "        for feature_index in range(num_features):\n",
    "            # grab just the features from the set\n",
    "            feature_values = dataset[:,feature_index]\n",
    "            #next isolate the possible thesholds by pulling out just the unique values from the dataset\n",
    "            possible_thesholds = np.unique(feature_values)\n",
    "            # Looping over the various thresholds to find the best one\n",
    "            for threshold in possible_thesholds:\n",
    "                dataset_left , dataset_right = self.split(dataset,feature_index, threshold)\n",
    "                # Check that children aren't null if it is then we are done we dont need to go any further if they are not we need to split the dataset further\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    # Splitting the dataset into the labels for the  left and right branches  \n",
    "                    y  = dataset[:,-1] \n",
    "                    left_y = dataset_left[:,-1]\n",
    "                    right_y =  dataset_right[:,-1]\n",
    "                    # Compute the gini coefficient to see if it is a improvement \n",
    "                    curr_info_gain = self.information_gain(y,left_y,right_y,\"gini\")\n",
    "                    # Check to see if this is the best split and if it is update all of our values \n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "        # Return this information \n",
    "        return best_split\n",
    "    \n",
    "    def split(self,dataset,feature_index,threshold):\n",
    "        # the function to actually split the data according to the possible different thresholds\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left , dataset_right\n",
    "    \n",
    "    def information_gain(self, parent, l_child, r_child, mode=\"entropy\"):\n",
    "            # computing the information gain \n",
    "        weight_l = len(l_child)/ len(parent)\n",
    "        weight_r = len(r_child)/ len(parent)\n",
    "        if mode == \"gini\":\n",
    "            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child)+weight_r*self.gini_index(r_child))\n",
    "        else:\n",
    "            gain = self.entropy(parent) - (weight_l*self.entropy(l_child)+weight_r*self.entropy(r_child))\n",
    "        return gain\n",
    "    \n",
    "    def entropy(self, y):\n",
    "        # a function that will compute entropy of the information sate\n",
    "        # pull all the different possible labels so we know how many different objects we are classifying. \n",
    "        class_labels = np.unique(y)\n",
    "        \n",
    "        entropy = 0 \n",
    "        # iterating through the different class labels\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y==cls])/len(y)\n",
    "            # This number will be closer to one the better the classification \n",
    "            entropy += - p_cls * np.log(p_cls)\n",
    "        return entropy\n",
    "\n",
    "    def gini_index(self, y):\n",
    "        #This method will work out the gini index of the tree\n",
    "        # pull all the different possible labels so we know how many different objects we are classifying. \n",
    "        class_labels = np.unique(y)\n",
    "        \n",
    "        gini = 0 \n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "            return 1 - gini \n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        # work out the value of each leaf\n",
    "        \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        #Print out the tree\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)  \n",
    "            \n",
    "    def fit(self, X, Y):\n",
    "        # actually fit the tree\n",
    "        # put the x and y sections of the data set together\n",
    "        dataset = np.concatenate((X,Y), axis=1)\n",
    "        #build the tree\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = [self.make_prediction(x,self.root) for x in X]\n",
    "        return predictions\n",
    "        \n",
    "    def make_prediction(self, x, tree):\n",
    "        # take our out of sample dataset and use it to make a prefiction to evaluate the model\n",
    "        if tree.value != None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature_index]\n",
    "        if feature_value <= tree.threshold:\n",
    "            return self.make_prediction(x,tree.left)\n",
    "        else: \n",
    "            return self.make_prediction(x, tree.right)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok I lied we are going to use sklearn one more time to do the train test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "Y = df.iloc[:, -1].values.reshape(-1,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.1, random_state=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_2 <= 1.9 ? 0.38888888888888884\n",
      " left:0.0\n",
      " right:X_3 <= 1.7 ? 0.6647392290249433\n",
      "  left:X_0 <= 7.0 ? 0.03797896709704286\n",
      "    left:1.0\n",
      "    right:2.0\n",
      "  right:2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier = Tree_classifier(min_samples_split=3, max_depth=2)\n",
    "classifier.fit(X_train,Y_train)\n",
    "classifier.print_tree()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
