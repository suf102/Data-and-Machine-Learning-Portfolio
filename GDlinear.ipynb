{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with some synthetic data to fit our regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a random seed so that we can go back to this data again. \n",
    "np.random.seed(7)\n",
    "# The real equation of the line that we will be working with.\n",
    "m = 0.4\n",
    "c = 9\n",
    "# The x labels of the data which be be evenly distributed across the interval [0,100] and data that is transformed by our desired linear function, with some random noise added.\n",
    "x = np.linspace(0,200,100)\n",
    "y = m*x + c + np.random.randn(len(x))*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to begin the liner regression by starting with a function we will do this by taking the mean slope of the data and the the mean bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_0 = np.sum((x - np.mean(x))*(y -np.mean(y)))/np.sum((x-np.mean(x))**2)\n",
    "c_0 = np.mean(y) - m_0*np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see this initial guess on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "plt.show\n",
    "plt.plot(x, x*m_0 + c_0 , '-r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the loss function, in this case it is a mean squared function that tells us how badly our regression function is doing  by computing the square of the distance between the prediction and the actual data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss( x_values, y_values, gradient, bias):\n",
    "    return (np.sum((x*gradient + bias - y)**2)/len(x_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function of our guess (that I deliberately messed up because it was too accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss(x,y,m_0,c_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to step the weights, this is done by differentiating with respect to the weights, then stepping them in the negative direction by the learning rates times the gradient. In this case I have manually performed the differentiation, as the model gets more complex a gradient vector would need to be calculated by some autodiff algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(m_start,c_start, learning_rate, x_values, y_values):\n",
    "    m_grad = (-2/len(x_values))*np.sum((x_values)*(y - (x_values*m_start + c_start )))\n",
    "    c_grad = (-2/len(x_values))*np.sum(y - (x_values*m_start + c_start ))\n",
    "    m_new = m_start - (learning_rate * m_grad)\n",
    "    c_new = c_start - (learning_rate * c_grad)\n",
    "    return m_new, c_new "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a function that will run the decent a number of times, we might want to specify this in one of two ways, the first is where we stop seeing improvements, or a set number of iterations. In this case I will use a set number of iterations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent_runner(m_start_guess, c_start_guess, x_values, y_values ,learning_rate, iterations):\n",
    "    losses = []\n",
    "    m_final = 0\n",
    "    c_final = 0\n",
    "    for i in range(iterations):\n",
    "        m_final, c_final  = optimizer(m_final,c_final,learning_rate,x_values,y_values ) \n",
    "        if i % 10 == 0 :\n",
    "            losses.append(loss(x_values, y_values, m_final, c_final))\n",
    "    return m_final , c_final, losses\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One will note here that the learning rate is very small, that is because the gradients are so high. When the learning rate is higher the algorithm begins to overshoot the optimization massively. The consequence of this is that we need a lot more iterations of the optimzer to be run in order to achieve the desired results. The number of iterations needed is so high that to stop too much memory being used the loss values are only be recorded every 10 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_finalfinal, c_finalfinal, loss_final = gradient_decent_runner(m_0, c_0, x,y,0.000001,10000000)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x, x*m_finalfinal + c_finalfinal, '-r')\n",
    "plt.show\n",
    "\n",
    "print(m_finalfinal)\n",
    "print(c_finalfinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the final losses to show how they decrease over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_final)\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
